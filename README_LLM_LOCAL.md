# Steam Deck Agent - Guide d'utilisation du LLM Local

## NouveautÃ©s

Votre application Steam Deck Agent dispose dÃ©sormais d'une option **LLM Local** qui permet d'interroger directement un modÃ¨le de langage local via Ollama, sans avoir Ã  copier-coller le JSON manuellement.

## FonctionnalitÃ©s ajoutÃ©es

### 1. Case Ã  cocher "LLM local"
- Une nouvelle option apparaÃ®t dans l'interface : **ðŸ¤– Utiliser LLM local (Ollama)**
- Lorsqu'elle est cochÃ©e, elle affiche les paramÃ¨tres de configuration Ollama

### 2. Configuration Ollama
Deux champs de configuration sont disponibles :
- **URL Ollama** : Par dÃ©faut `http://localhost:11434`
- **ModÃ¨le** : Par dÃ©faut `llama3.2` (vous pouvez utiliser `mistral`, `llama2`, `codellama`, etc.)

### 3. Fonctionnement
Lorsque le mode LLM local est activÃ© :
1. Vous cliquez sur "ðŸ†• Nouveaux jeux" ou "ðŸŽ® Jeux dÃ©jÃ  Ã  jouer"
2. L'application envoie automatiquement la requÃªte Ã  votre Ollama local
3. Les suggestions s'affichent directement dans l'interface
4. Vous pouvez les sauvegarder comme avant

## Installation et configuration d'Ollama

### Ã‰tape 1 : Installer Ollama

**Linux :**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**macOS :**
```bash
brew install ollama
```

**Windows :**
TÃ©lÃ©chargez l'installateur sur https://ollama.com/download

### Ã‰tape 2 : Lancer Ollama

```bash
ollama serve
```

Par dÃ©faut, Ollama dÃ©marre sur `http://localhost:11434`

### Ã‰tape 3 : TÃ©lÃ©charger un modÃ¨le

```bash
# ModÃ¨le recommandÃ© pour cette tÃ¢che (lÃ©ger et efficace)
ollama pull llama3.2

# Alternatives
ollama pull mistral
ollama pull llama2
ollama pull codellama
```

### Ã‰tape 4 : Tester

```bash
ollama run llama3.2
```

## Utilisation dans l'application

1. **Lancez votre serveur FastAPI** :
   ```bash
   python main.py
   ```

2. **AccÃ©dez Ã  l'interface** : `http://localhost:8000`

3. **Chargez votre bibliothÃ¨que Steam**

4. **Cochez "ðŸ¤– Utiliser LLM local (Ollama)"**

5. **VÃ©rifiez la configuration** :
   - URL : `http://localhost:11434`
   - ModÃ¨le : `llama3.2` (ou votre modÃ¨le prÃ©fÃ©rÃ©)

6. **Cliquez sur un des boutons de suggestion** :
   - ðŸ†• Nouveaux jeux
   - ðŸŽ® Jeux dÃ©jÃ  Ã  jouer

7. **Attendez la gÃ©nÃ©ration** (un spinner s'affiche)

8. **Les suggestions apparaissent automatiquement !**

9. **Sauvegardez-les** avec le bouton "SAUVEGARDER"

## ModÃ¨les recommandÃ©s

| ModÃ¨le | Taille | Performance | Recommandation |
|--------|--------|-------------|----------------|
| `llama3.2` | ~2GB | Rapide | â­ RecommandÃ© pour usage quotidien |
| `mistral` | ~4GB | TrÃ¨s bon | â­ Excellent Ã©quilibre |
| `llama2` | ~4GB | Bon | Alternative stable |
| `llama3.1` | ~5GB | Excellent | Si vous avez la RAM |
| `codellama` | ~4GB | SpÃ©cialisÃ© code | Pour suggestions techniques |

## Avantages du mode LLM local

âœ… **Pas de copier-coller** : Tout est automatique
âœ… **PrivÃ©** : Vos donnÃ©es restent sur votre machine
âœ… **Gratuit** : Pas de coÃ»t d'API
âœ… **Rapide** : RÃ©ponse en quelques secondes
âœ… **Personnalisable** : Choisissez votre modÃ¨le prÃ©fÃ©rÃ©
âœ… **Hors ligne** : Fonctionne sans internet (aprÃ¨s tÃ©lÃ©chargement du modÃ¨le)

## DÃ©pannage

### Erreur de connexion
**ProblÃ¨me** : `Impossible de se connecter Ã  Ollama`

**Solutions** :
1. VÃ©rifiez qu'Ollama est lancÃ© : `ollama serve`
2. VÃ©rifiez l'URL dans la config (par dÃ©faut : `http://localhost:11434`)
3. Testez manuellement : `curl http://localhost:11434/api/version`

### Timeout
**ProblÃ¨me** : `Timeout lors de la requÃªte`

**Solutions** :
1. Utilisez un modÃ¨le plus lÃ©ger (ex: `llama3.2` au lieu de `llama3.1`)
2. RÃ©duisez la taille de votre bibliothÃ¨que Steam (filtrez avant)
3. Augmentez le timeout dans `main.py` (ligne avec `timeout=120.0`)

### RÃ©ponse invalide
**ProblÃ¨me** : `Impossible de parser la rÃ©ponse JSON`

**Solutions** :
1. Le modÃ¨le n'a peut-Ãªtre pas respectÃ© le format JSON
2. Essayez un autre modÃ¨le (Mistral est gÃ©nÃ©ralement trÃ¨s bon pour du JSON)
3. Regardez les logs du serveur FastAPI pour voir la rÃ©ponse brute

### ModÃ¨le non trouvÃ©
**ProblÃ¨me** : `Model not found`

**Solution** :
```bash
ollama pull llama3.2
```

## Comparaison des modes

| FonctionnalitÃ© | Mode Manuel | Mode LLM Local |
|----------------|-------------|----------------|
| Copier-coller | âœ… Requis | âŒ Automatique |
| Internet requis | âœ… Oui (pour le LLM externe) | âŒ Non |
| CoÃ»t | Variable selon LLM | âœ… Gratuit |
| Vitesse | DÃ©pend du LLM externe | âš¡ Rapide (local) |
| ConfidentialitÃ© | DonnÃ©es envoyÃ©es | âœ… 100% local |
| Configuration | Aucune | Installation Ollama |

## Exemple de workflow complet

```bash
# 1. Installer et lancer Ollama
ollama serve

# 2. Dans un autre terminal, tÃ©lÃ©charger le modÃ¨le
ollama pull llama3.2

# 3. Lancer votre serveur
python main.py

# 4. Ouvrir le navigateur
# http://localhost:8000

# 5. Dans l'interface :
#    - Charger la bibliothÃ¨que Steam
#    - Cocher "LLM local"
#    - Cliquer sur "Nouveaux jeux"
#    - Attendre les suggestions
#    - Sauvegarder !
```

## Architecture technique

### Nouveau endpoint FastAPI
```python
@app.post("/api/ollama/generate")
async def ollama_generate(request: OllamaRequest):
    # Envoie la requÃªte Ã  Ollama
    # Parse la rÃ©ponse JSON
    # Retourne les suggestions
```

### Frontend JavaScript
```javascript
async function queryLocalLLM(prompt) {
    // Appelle l'endpoint /api/ollama/generate
    // Affiche le spinner
    // RÃ©cupÃ¨re et affiche les suggestions
}
```

## Notes importantes

1. **Premier lancement** : Le premier appel peut Ãªtre lent (chargement du modÃ¨le en mÃ©moire)
2. **MÃ©moire RAM** : Assurez-vous d'avoir assez de RAM pour le modÃ¨le (2-8GB selon le modÃ¨le)
3. **Format JSON** : Les modÃ¨les rÃ©cents (Llama 3.2, Mistral) sont trÃ¨s bons pour gÃ©nÃ©rer du JSON valide
4. **BibliothÃ¨que volumineuse** : Si vous avez beaucoup de jeux (>500), le prompt peut Ãªtre long. Filtrez d'abord !

## Support

Si vous rencontrez des problÃ¨mes :
1. VÃ©rifiez les logs du serveur FastAPI
2. VÃ©rifiez les logs d'Ollama : `ollama logs`
3. Testez Ollama directement : `ollama run llama3.2`

Bon gaming ! ðŸŽ®
